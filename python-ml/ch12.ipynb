{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 4 10 3\n",
      "10 5 3 1 6 4\n"
     ]
    }
   ],
   "source": [
    "#example 12-1\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "data = range(1, 11)   #1 - 10\n",
    "\n",
    "g1 = resample(data, replace=False, n_samples=4)\n",
    "g2 = resample(data, replace=False, n_samples=6)\n",
    "print(*g1)\n",
    "print(*g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 2 6 5\n",
      "2 7 7 1 7\n"
     ]
    }
   ],
   "source": [
    "#example 12-2\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "data = range(1, 11)   #1 - 10\n",
    "\n",
    "g1 = resample(data, replace=True, n_samples=5)\n",
    "g2 = resample(data, replace=True, n_samples=5)\n",
    "print(*g1)\n",
    "print(*g2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Score: 0.88\n",
      "Decision Tree Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "#example 12-3\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=100, n_features=5, \n",
    "                           n_classes=2, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "score = forest.score(x_test, y_test)\n",
    "print('Random Forest Score:', score)\n",
    "\n",
    "#--------------------------------------------\n",
    "#ลองเปรียบเทียบกับ Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "tree_score = tree.score(x_test, y_test)\n",
    "print('Decision Tree Score:', tree_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9895614886775905\n"
     ]
    }
   ],
   "source": [
    "#example 12-4\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "x, y = make_regression(n_samples=500, n_features=3, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "score = forest.score(x_test, y_test)\n",
    "print('Score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.864\n"
     ]
    }
   ],
   "source": [
    "#example 12-5\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=500, n_features=5, \n",
    "                           n_classes=2, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "adb = AdaBoostClassifier()\n",
    "adb.fit(x_train, y_train)\n",
    "\n",
    "score = adb.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9650526769618516\n"
     ]
    }
   ],
   "source": [
    "#example 12-6\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "x, y = make_regression(n_samples=500, n_features=3, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "adb = AdaBoostRegressor()\n",
    "adb.fit(x_train, y_train)\n",
    "\n",
    "score = adb.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.904\n"
     ]
    }
   ],
   "source": [
    "#example 12-7\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=500, n_features=5, \n",
    "                           n_classes=2, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "gb = GradientBoostingClassifier(learning_rate=0.1, random_state=0)\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "score = gb.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.940616842659719\n"
     ]
    }
   ],
   "source": [
    "#example 12-8\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "x, y = make_regression(n_samples=500, n_features=5, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate=0.1, random_state=0)\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "score = gb.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.972\n"
     ]
    }
   ],
   "source": [
    "#example 12-9\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=5, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "xgboost = XGBClassifier(use_label_encoder=False, \n",
    "                        objective='binary:logitraw')\n",
    "\n",
    "xgboost.fit(x_train, y_train)\n",
    "\n",
    "score = xgboost.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9457058861430043\n"
     ]
    }
   ],
   "source": [
    "#example 12-10\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "x, y = make_regression(n_samples=1000, n_features=5, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "xgboost = XGBRegressor(objective='reg:squarederror')\n",
    "xgboost.fit(x_train, y_train)\n",
    "\n",
    "score = xgboost.score(x_test, y_test)\n",
    "print('Score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Score: 0.9704\n",
      "XGBoost Time: 0.37998294830322266 - Second\n",
      "\n",
      "Gradient Boosting Score: 0.9696\n",
      "Gradient Boosting Time: 0.6512594223022461 - Second\n"
     ]
    }
   ],
   "source": [
    "#example 12-11\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "\n",
    "x, y = make_classification(n_samples=5000, n_features=5, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "#------------- XGBoost -------------\n",
    "xgb_start = time.time()\n",
    "xgboost = XGBClassifier(use_label_encoder=False, \n",
    "                        objective='binary:logitraw')\n",
    "\n",
    "xgboost.fit(x_train, y_train)\n",
    "\n",
    "score = xgboost.score(x_test, y_test)\n",
    "xgb_end = time.time()\n",
    "\n",
    "print('XGBoost Score:', score)\n",
    "print('XGBoost Time:', (xgb_end - xgb_start), '- Second')\n",
    "print()\n",
    "\n",
    "#---------- Gradient Boost ---------\n",
    "gb_start = time.time()\n",
    "gb = GradientBoostingClassifier(learning_rate=0.1, random_state=0)\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "score = gb.score(x_test, y_test)\n",
    "gb_end = time.time()\n",
    "\n",
    "print('Gradient Boosting Score:', score)\n",
    "print('Gradient Boosting Time:', (gb_end - gb_start), '- Second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.932\n"
     ]
    }
   ],
   "source": [
    "#example 12-12\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=4, \n",
    "                           n_classes=2, random_state=100)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=100)\n",
    "\n",
    "#กำหนดโมเดลทั้งหมดในชั้น Base Learners ไว้ในลิสต์\n",
    "#จากนั้นกำหนดชื่ออ้างอิงและสร้างอินสแตนซ์ของแต่ละโมเดล\n",
    "base_estimators = [\n",
    "    ('tree', DecisionTreeClassifier(random_state=100)),\n",
    "    ('svm', SVC(kernel='linear')),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('bayes', GaussianNB()),\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(estimators=base_estimators, \n",
    "                           final_estimator=LogisticRegression())  #ปกติก็เป็นค่าดีฟอลต์อยู่แล้ว\n",
    "\n",
    "stack.fit(x_train, y_train)\n",
    "\n",
    "score = stack.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.951659229530975\n"
     ]
    }
   ],
   "source": [
    "#example 12-13\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "x, y = make_regression(n_samples=1000, n_features=5, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "adb = AdaBoostRegressor()\n",
    "gb = GradientBoostingRegressor(learning_rate=0.1, random_state=0)\n",
    "xgb = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "base_estimators = [\n",
    "    ('forest', forest),\n",
    "    ('adb', adb),\n",
    "    ('gb', gb)\n",
    "]\n",
    "\n",
    "stack = StackingRegressor(estimators=base_estimators, \n",
    "                          final_estimator=xgb)\n",
    "\n",
    "stack.fit(x_train, y_train)\n",
    "\n",
    "score = stack.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.916\n"
     ]
    }
   ],
   "source": [
    "#example 12-14\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=4, \n",
    "                           n_classes=2, random_state=100)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=100)\n",
    "\n",
    "estimators = [\n",
    "    ('tree', DecisionTreeClassifier(random_state=100)),\n",
    "    ('svm', SVC(kernel='linear', probability=True)),   #*****\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('bayes', GaussianNB()),\n",
    "]\n",
    "\n",
    "stack = VotingClassifier(estimators, voting='hard')   #voting='soft'\n",
    "stack.fit(x_train, y_train)\n",
    "\n",
    "score = stack.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9376383308254168\n"
     ]
    }
   ],
   "source": [
    "#example 12-15\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "x, y = make_regression(n_samples=1000, n_features=5, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "adb = AdaBoostRegressor()\n",
    "gb = GradientBoostingRegressor(learning_rate=0.1, random_state=0)\n",
    "xgb = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "estimators = [\n",
    "    ('forest', forest),\n",
    "    ('adb', adb),\n",
    "    ('gb', gb),\n",
    "    ('xbg', xgb)\n",
    "]\n",
    "\n",
    "voting = VotingRegressor(estimators)\n",
    "voting.fit(x_train, y_train)\n",
    "\n",
    "score = voting.score(x_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9533333333333334\n",
      "Best Params: {'n_neighbors': 7}\n",
      "Optimal K: 7\n",
      "Prediction:  1\n"
     ]
    }
   ],
   "source": [
    "#example 12-16\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "x, y = make_classification(n_samples=200, n_features=4, \n",
    "                           n_classes=2, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "params = { 'n_neighbors': range(1, 51, 2) }  #1, 3, 5, ...\n",
    "\n",
    "grid = GridSearchCV(estimator=knn, param_grid=params)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print('Best Score:', grid.best_score_)\n",
    "print('Best Params:', grid.best_params_)\n",
    "print('Optimal K:', grid.best_params_['n_neighbors'])\n",
    "\n",
    "#แนวทางการนำ best_estimator ไปใช้งานต่อ เช่น การทำนายผล\n",
    "\n",
    "#print(x_train[:5])\n",
    "#print(y_train[:5])\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "x_predict = [[1.5], [0.50], [0.15], [1.5]]\n",
    "x_predict = np.array(x_predict).reshape(-1, 4)\n",
    "\n",
    "y_predict = best_knn.predict(x_predict)\n",
    "print('Prediction: ', y_predict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Best Score: 0.9613333333333334\n",
      "KNN Best Params: {'n_neighbors': 9}\n",
      "\n",
      "Random Forest Best Score: 0.9626666666666667\n",
      "Random Forest Best Params: {'criterion': 'gini', 'n_estimators': 65}\n",
      "\n",
      "SVM Best Score: 0.9653333333333334\n",
      "SVM Best Params: {'C': 5, 'gamma': 3, 'kernel': 'rbf'}\n",
      "\n",
      "Total Time: 89.6127712726593 - Second\n"
     ]
    }
   ],
   "source": [
    "#example 12-17\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=4, \n",
    "                           n_classes=2, random_state=0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "#----- KNN -----\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {'n_neighbors': range(1, 50, 2)}\n",
    "\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=knn_params)\n",
    "knn_grid.fit(x_train, y_train)\n",
    "\n",
    "print('KNN Best Score:', knn_grid.best_score_)\n",
    "print('KNN Best Params:', knn_grid.best_params_)\n",
    "print()\n",
    "\n",
    "#----- Random Forest -----\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest_params = {\n",
    "    'n_estimators': range(10, 100, 5),\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    #'max_depth': range(1, 80)\n",
    "}\n",
    "forest_grid = GridSearchCV(estimator=forest, param_grid=forest_params)\n",
    "forest_grid.fit(x_train, y_train)\n",
    "\n",
    "print ('Random Forest Best Score:', forest_grid.best_score_)\n",
    "print ('Random Forest Best Params:', forest_grid.best_params_)\n",
    "print()\n",
    "\n",
    "#----- SVM -----\n",
    "svm = SVC()\n",
    "svm_params = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 5, 10, 50]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 5, 10, 50], 'gamma': range(1, 10)},\n",
    "    {'kernel': ['poly'], 'C': [0.1, 1, 5, 10, 50], 'degree': range(1, 10)},\n",
    "]\n",
    "\n",
    "svm_grid = GridSearchCV(estimator=svm, param_grid=svm_params)\n",
    "svm_grid.fit(x_train, y_train)\n",
    "\n",
    "print('SVM Best Score:', svm_grid.best_score_)\n",
    "print('SVM Best Params:', svm_grid.best_params_)\n",
    "print()\n",
    "#---------------------------------------\n",
    "\n",
    "time_end = time.time()\n",
    "print(f'Total Time: {time_end - time_start} - Second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best score:  0.8666666666666668\n",
      "KNN best params:  {'n_neighbors': 13}\n",
      "\n",
      "Random Forest best score:  0.8560000000000001\n",
      "Random Forest best params:  {'n_estimators': 75, 'criterion': 'gini'}\n",
      "\n",
      "SVM best score: 0.8706666666666667\n",
      "SVM best params: {'kernel': 'rbf', 'gamma': 2, 'C': 1}\n",
      "\n",
      "Total time: 4.082298040390015 - second\n"
     ]
    }
   ],
   "source": [
    "#example 12-18\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=4, \n",
    "                           n_classes=2, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "#----- KNN -----\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {'n_neighbors': range(1, 50, 2)}\n",
    "random_knn = RandomizedSearchCV(estimator=knn, param_distributions=knn_params)\n",
    "random_knn.fit(x_train, y_train)\n",
    "\n",
    "print('KNN best score: ', random_knn.best_score_)\n",
    "print('KNN best params: ', random_knn.best_params_)\n",
    "print()\n",
    "\n",
    "#----- Random Forest -----\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest_params = {\n",
    "    'n_estimators': range(10, 100, 5),\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    #'max_depth': range(1, 80)\n",
    "}\n",
    "random_forest = RandomizedSearchCV(estimator=forest, param_distributions=forest_params, n_iter=4)\n",
    "random_forest.fit(x_train, y_train)\n",
    "\n",
    "print('Random Forest best score: ', random_forest.best_score_)\n",
    "print('Random Forest best params: ', random_forest.best_params_)\n",
    "print()\n",
    "\n",
    "#----- SVM -----\n",
    "svm = SVC()\n",
    "svm_params = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 5, 10, 50]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 5, 10, 50], 'gamma': range(1, 10)},\n",
    "    {'kernel': ['poly'], 'C': [0.1, 1, 5, 10, 50], 'degree': range(1, 10)},\n",
    "]\n",
    "\n",
    "random_svm = RandomizedSearchCV(estimator=svm, param_distributions=svm_params)\n",
    "random_svm.fit(x_train, y_train)\n",
    "\n",
    "print('SVM best score:', random_svm.best_score_)\n",
    "print('SVM best params:', random_svm.best_params_)\n",
    "print()\n",
    "#---------------------------------------\n",
    "\n",
    "time_end = time.time()\n",
    "print(f'Total time: {time_end - time_start} - second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score: 0.96\n",
      "Random Forest Score: 0.952\n",
      "SVM Score: 0.952\n",
      "Stacking Score: 0.968\n"
     ]
    }
   ],
   "source": [
    "#example 12-19\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "x, y = make_classification(n_samples=500, n_features=4, \n",
    "                           n_classes=2, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n",
    "\n",
    "#----- KNN -----\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {'n_neighbors': range(1, 30, 2)}\n",
    "knn_random = RandomizedSearchCV(estimator=knn, param_distributions=knn_params)\n",
    "knn_random.fit(x_train, y_train)\n",
    "\n",
    "print('KNN Score:', knn_random.score(x_test, y_test))\n",
    "knn_best = knn_random.best_estimator_\n",
    "\n",
    "#----- Random Forest -----\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest_params = {\n",
    "    'n_estimators': range(10, 100, 5),\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    #'max_depth': range(1, 80)\n",
    "}\n",
    "forest_random = RandomizedSearchCV(estimator=forest, param_distributions=forest_params, n_iter=4)\n",
    "forest_random.fit(x_train, y_train)\n",
    "\n",
    "print('Random Forest Score:', forest_random.score(x_test, y_test))\n",
    "forest_best = forest_random.best_estimator_\n",
    "\n",
    "#----- SVM -----\n",
    "svm = SVC()\n",
    "svm_params = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 5, 10, 50]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 5, 10, 50], 'gamma': range(1, 10)},\n",
    "    {'kernel': ['poly'], 'C': [0.1, 1, 5, 10, 50], 'degree': range(1, 10)},\n",
    "]\n",
    "\n",
    "svm_random = RandomizedSearchCV(estimator=svm, param_distributions=svm_params)\n",
    "svm_random.fit(x_train, y_train)\n",
    "\n",
    "print('SVM Score:', svm_random.score(x_test, y_test))\n",
    "svm_best = svm_random.best_estimator_\n",
    "#---------------------------------------\n",
    "\n",
    "#นำออบเจ็กต์ของแต่ละโมเดลที่ประกอบด้วยพารามิเตอร์ที่ดีที่สุด\n",
    "#มาใช้ร่วมกับ Ensemble (ในที่นี้คือ StackingClassifier)\n",
    "estimators = [\n",
    "    ('knn', knn_best),\n",
    "    ('forest', forest_best),\n",
    "    ('svm', svm_best),\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(estimators,\n",
    "                           final_estimator=LogisticRegression())\n",
    "\n",
    "stack.fit(x_train, y_train)\n",
    "\n",
    "score = stack.score(x_test, y_test)\n",
    "print('Stacking Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Params: {'n_estimators': 20}\n",
      "MLP Best Params: {'hidden_layer_sizes': 6}\n",
      "Prediction: 15.70\n",
      "Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "#example 12-20\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "\n",
    "df = pd.read_excel(r'data\\car_miles_per_gallon.xlsx')\n",
    "df.dropna(inplace=True)\n",
    "#with pd.option_context('display.max_rows', 10): display(df)\n",
    "\n",
    "#คอลัมน์ที่จะใช้ Train Model: \n",
    "#Cylinders, Displacement, Horsepower, Weight, Acceleration\n",
    "x = df.iloc[:, 3:8]  \n",
    "y = df['MPG']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "#----- Random Forest -----\n",
    "forest = RandomForestRegressor()\n",
    "forest_params = { 'n_estimators': range(10, 100, 5) }\n",
    "forest_grid = GridSearchCV(estimator=forest, param_grid=forest_params)\n",
    "forest_grid.fit(x_train, y_train)\n",
    "\n",
    "print('Random Forest Best Params:', forest_grid.best_params_)\n",
    "forest_best = forest_grid.best_estimator_\n",
    "\n",
    "#----- Multi-layer Perceptron -----\n",
    "mlp = MLPRegressor(activation='relu',\n",
    "                   solver='lbfgs',\n",
    "                   max_iter=10000,\n",
    "                   random_state=0)\n",
    "\n",
    "mlp_params = { 'hidden_layer_sizes': range(3, 7) }   #ทดสอบ 3 - 6 นิวรอน\n",
    "\n",
    "mlp_grid = GridSearchCV(estimator=mlp, param_grid=mlp_params)\n",
    "mlp_grid.fit(x_train, y_train)\n",
    "\n",
    "print('MLP Best Params:', mlp_grid.best_params_)\n",
    "mlp_best = mlp_grid.best_estimator_\n",
    "\n",
    "#นำ Best Estimator มาใช้ร่วมกับ Voting\n",
    "estimators = [\n",
    "    ('forest', forest_best),\n",
    "    ('mlp', mlp_best),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror'))\n",
    "]\n",
    "\n",
    "voting = VotingRegressor(estimators)\n",
    "voting.fit(x_train, y_train)\n",
    "\n",
    "#ข้อมูลที่จะทำนายผล ก็ต้องแปลงสเกลเช่นกัน\n",
    "x_predict = [[6, 400, 150, 4000, 9]]\n",
    "x_predict = scaler.transform(x_predict)\n",
    "\n",
    "predict = voting.predict(x_predict)\n",
    "\n",
    "print('Prediction:', '{:.2f}'.format(predict[0]))\n",
    "print('Score:', '{:.2f}'.format(voting.score(x_test, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
